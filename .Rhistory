arr.Z = AR_reml(Z ~ t.scale) #Z-transformed time trends
}
time.trends.Z[tx,1] = strsplit(u.LLS2[l],'_')[[1]][1] #LTER
time.trends.Z[tx,2] = strsplit(u.LLS2[l],'_')[[1]][2] #Site
time.trends.Z[tx,3] = paste0(strsplit(u.LLS2[l],'_')[[1]][3:length(strsplit(u.LLS2[l],'_')[[1]])],collapse='_') #Species
time.trends.Z[tx,4] = add.coef.Z[1] #MSE
time.trends.Z[tx,5] = add.coef.Z[2] #b
time.trends.Z[tx,6] = add.coef.Z[3] #coef_int
time.trends.Z[tx,7] = add.coef.Z[4] #coef_slope
time.trends.Z[tx,8] = add.coef.Z[5] #Pr_int
time.trends.Z[tx,9] = add.coef.Z[6] #Pr_slope
time.trends.Z[tx,10] = add.coef.Z[7] #logLik
time.trends.Z[tx,11] = length(which(!is.na(Z))) #length of time series
time.trends.Z[tx,12] = lls.years.stretch[1] #first year in time series
time.trends.Z[tx,13] = lls.years.stretch[length(lls.years.stretch)] #last year in time series
time.trends.Z[tx,14] = lls.data$Feeding[1]
time.trends.Z[tx,15] = lls.data$Habitat[1]
time.trends.Z[tx,16] = lls.data$Pollinator[1]
time.trends.Z[tx,17] = lls.data$Order[1]
tx = tx + 1
# Plot high quality time series
png(paste0(gsub('\\?','',u.LLS[l]),'.png'))
plot(t.scale,Z,type='l',main=u.LLS2[l],xlab='Scaled time',ylab='Z-transformed abundance',lwd=2)
abline(a=add.coef.Z[3],b=add.coef.Z[4],lty=2,col='red',lwd=1.5)
legend('topright',legend=c(paste0('slope = ',round(add.coef.Z[4],2)),paste0('length = ',length(which(!is.na(Z)))),paste0('autocor = ',round(add.coef.Z[2],2)),paste0('logLik = ',round(add.coef.Z[7],2))),ncol=1)
dev.off()
} else {
#ignore low quality time series
}
}
head(time.trends.Z)
LLS
LLS = paste(data1$LTER.site,data1$Locale,data1$Species.code,sep='_') #unique LTER by sub-site by species combinations
u.LLS = unique(LLS)
LLS.count = apply(array(u.LLS),1,function(x){d2=data1[which(LLS==x),];length(which(d2$Abundance!=0))})
u.LLS2 = u.LLS[which(LLS.count>0)]
time.trends.Z = data.frame('LTER'=NA,'Site'=NA,'Species'=NA,'MSE'=-999,'b'=-999,'coef_int'=-999,'coef_slope'=-999,'Pr_int'=-999,'Pr_slope'=-999,'logLik'=-999,'length'=-999,'Y1'=NA,'Y2'=NA,'Feeding'=NA,'Habitat'=NA,'Pollinator'=NA,'Order'=NA)
tx = 1
u.LLS
u.LLS2
for (l in 1:length(u.LLS2)){
print(l)
lls.data = data1[which(LLS==u.LLS2[l]),]
## This part I added in to try and keep the data to just the 2015 data used in Crossley.
# lls.data = lls.data[which(lls.data$Year <= 2015), ]
## It seems like this is just recreating the site and study name? Not sure why
LL = paste0(strsplit(u.LLS2[l],'_')[[1]][1:2],collapse='_') #trim "no sample" years
## Removed a harvard forest ants specific processing step
### Quality threshold
## Checking if there are more than 3 years where there are actual measurements for abundance and that of those non-NA
## years there are at least a few with non-zero values for abundance measured.
if ( (length(which(!is.na(lls.data$Abundance))) > 3) & (length(which(lls.data$Abundance[!is.na(lls.data$Abundance)] > 0)) > 0) ){
###
## This is measuring the non-zero minimum
cmin = min(lls.data$Abundance[which(lls.data$Abundance>0)],na.rm=T)
## I don't understand how cmin would be NA after their preprocessing steps? Should be at least 1 non-zero values?
## I suppose this is their way of transforming so that they can take the log of the data
if (is.na(cmin)){
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5
} else {
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5 * cmin #replace zeroes with 0.5*minimum abundance value in this time series
}
lls.data$Abundance = log(lls.data$Abundance) #log-transform abundances
if (length(which(lls.data$Abundance==0))==length(lls.data$Abundance)){ #abundances are all = 1
add.coef = add.coef.Z = c(NA,NA,0,0,NA,NA,NA)
} else {
lls.years = sort(lls.data$Year)
lls.years.stretch = seq(lls.years[1],lls.years[length(lls.years)],1) #expand time series to include missing years
llsy.match = match(lls.years.stretch,lls.years)
X = llsy.match
## Here they are replacing the years with abundances and then having NAs for the years missing data in the expanded set
X[which(!is.na(X))] = lls.data$Abundance[X[which(!is.na(X))]] #fill-in abundance values in the expanded time series
## For some reason R is not allowing me to keep the format the same as they had it, so instead of X - mean() I am
## trying -mean() + X
Z = (-mean(X, na.rm=T) + X)/sd(X, na.rm=T) # z-transform
t.scale = 1:length(lls.years.stretch)
## Here they are standardizing timescales, I assume to make so that it is the same across different lengths of monitoring.
## It says it is transforming it between 0 and 1 but I think that it would need to be divided by max(t.scale) - min(t.scale)
# for that to be the case. Instead its transforming to ~almost~ 1.
t.scale = (t.scale-min(t.scale))/max(t.scale) #original transform: scale between 0 and 1
arr.Z = AR_reml(Z ~ t.scale) #Z-transformed time trends
}
time.trends.Z[tx,1] = strsplit(u.LLS2[l],'_')[[1]][1] #LTER
time.trends.Z[tx,2] = strsplit(u.LLS2[l],'_')[[1]][2] #Site
time.trends.Z[tx,3] = paste0(strsplit(u.LLS2[l],'_')[[1]][3:length(strsplit(u.LLS2[l],'_')[[1]])],collapse='_') #Species
time.trends.Z[tx,4] = add.coef.Z[1] #MSE
time.trends.Z[tx,5] = add.coef.Z[2] #b
time.trends.Z[tx,6] = add.coef.Z[3] #coef_int
time.trends.Z[tx,7] = add.coef.Z[4] #coef_slope
time.trends.Z[tx,8] = add.coef.Z[5] #Pr_int
time.trends.Z[tx,9] = add.coef.Z[6] #Pr_slope
time.trends.Z[tx,10] = add.coef.Z[7] #logLik
time.trends.Z[tx,11] = length(which(!is.na(Z))) #length of time series
time.trends.Z[tx,12] = lls.years.stretch[1] #first year in time series
time.trends.Z[tx,13] = lls.years.stretch[length(lls.years.stretch)] #last year in time series
time.trends.Z[tx,14] = lls.data$Feeding[1]
time.trends.Z[tx,15] = lls.data$Habitat[1]
time.trends.Z[tx,16] = lls.data$Pollinator[1]
time.trends.Z[tx,17] = lls.data$Order[1]
tx = tx + 1
# Plot high quality time series
png(paste0(gsub('\\?','',u.LLS[l]),'.png'))
plot(t.scale,Z,type='l',main=u.LLS2[l],xlab='Scaled time',ylab='Z-transformed abundance',lwd=2)
abline(a=add.coef.Z[3],b=add.coef.Z[4],lty=2,col='red',lwd=1.5)
legend('topright',legend=c(paste0('slope = ',round(add.coef.Z[4],2)),paste0('length = ',length(which(!is.na(Z)))),paste0('autocor = ',round(add.coef.Z[2],2)),paste0('logLik = ',round(add.coef.Z[7],2))),ncol=1)
dev.off()
} else {
#ignore low quality time series
}
}
head(time.trends.Z)
AR_reml <- function(formula, data = list()) {
AR_reml_funct <- function(par, x, u) {
# b is a set parameter
b <- par
n.obs <- length(x)
# q is the number of columns that are being considered in that data relevant to the model
q <- dim(u)[2]
# A n by n matrix with dimensions based on the number of observations. Populated with zeros and then
# 1s on the diagonal.
B <- diag(n.obs)
# populates the diagonal under the main diagonal with negative the set parameter.
diag(B[-1, ]) <- -b
# New matrix of zeroes n by n dimensions with zeros on the principal diagonal.
iS <- diag(n.obs)
# Changes the first value based on the set parameter value fed in.
iS[1, 1] <- (1 - b^2)
# Ultimately the weirdness with converting the first value in iS just leads to it allowing that value to
# be 1 when converted in this way.
iV <- t(B) %*% iS %*% B
logdetV <- -determinant(iV)$modulus[1]
coef <- solve(t(u) %*% iV %*% u, t(u) %*% iV %*% x)
H <- x - u %*% coef
s2 <- (t(H) %*% iV %*% H)/(n.obs - q)
LL <- 0.5 * ((n.obs - q) * log(s2) + logdetV + determinant(t(u) %*% iV %*% u)$modulus[1] + (n.obs - q))
#show(c(LL,b))
return(LL)
}
mf <- model.frame(formula = formula, data = data)
u <- model.matrix(attr(mf, "terms"), data = mf)
x <- model.response(mf)
q <- dim(u)[2]
opt <- optim(fn = AR_reml_funct, par = 0.2, method = "Brent", upper = 1, lower = -1, control = list(maxit = 10^4), x = x, u = u)
b <- opt$par
n.obs <- length(x)
q <- dim(u)[2]
B <- diag(n.obs)
diag(B[-1, ]) <- -b
iS <- diag(n.obs)
iS[1, 1] <- (1 - b^2)
iV <- t(B) %*% iS %*% B
logdetV <- -determinant(iV)$modulus[1]
coef <- solve(t(u) %*% iV %*% u, t(u) %*% iV %*% x)
H <- x - u %*% coef
MSE <- as.numeric((t(H) %*% iV %*% H)/(n.obs - q))
s2coef <- MSE * solve(t(u) %*% iV %*% u)
Pr <- 1:q
for (i in 1:q) Pr[i] <- 2 * pt(abs(coef[i])/s2coef[i, i]^0.5, df = n.obs - q, lower.tail = F)
logLik <- 0.5 * (n.obs - q) * log(2 * pi) + determinant(t(u) %*% u)$modulus[1] - opt$value
return(list(MSE = MSE, b = b, coef = coef, s2coef = s2coef, Pr = Pr, logLik = logLik))
}
data1 = read.csv('PerSpecies_Abundance_LTER_annotated.csv',as.is=T,check.names=F,header=T)
data1$LL = paste(data1$LTER.site,data1$Locale,sep='_')
sp.count = apply(array(unique(data1$LL)),1,function(x){length(unique(data1$Species.code[which(data1$LL==x)]))})
LLS = paste(data1$LTER.site,data1$Locale,data1$Species.code,sep='_') #unique LTER by sub-site by species combinations
u.LLS = unique(LLS)
LLS.count = apply(array(u.LLS),1,function(x){d2=data1[which(LLS==x),];length(which(d2$Abundance!=0))})
u.LLS2 = u.LLS[which(LLS.count>0)]
time.trends.Z = data.frame('LTER'=NA,'Site'=NA,'Species'=NA,'MSE'=-999,'b'=-999,'coef_int'=-999,'coef_slope'=-999,'Pr_int'=-999,'Pr_slope'=-999,'logLik'=-999,'length'=-999,'Y1'=NA,'Y2'=NA,'Feeding'=NA,'Habitat'=NA,'Pollinator'=NA,'Order'=NA)
tx = 1
# this goes through each species in each site
for (l in 1:length(u.LLS2)){
print(l)
lls.data = data1[which(LLS==u.LLS2[l]),]
## This part I added in to try and keep the data to just the 2015 data used in Crossley.
# lls.data = lls.data[which(lls.data$Year <= 2015), ]
## It seems like this is just recreating the site and study name? Not sure why
LL = paste0(strsplit(u.LLS2[l],'_')[[1]][1:2],collapse='_') #trim "no sample" years
## Removed a harvard forest ants specific processing step
### Quality threshold
## Checking if there are more than 3 years where there are actual measurements for abundance and that of those non-NA
## years there are at least a few with non-zero values for abundance measured.
if ( (length(which(!is.na(lls.data$Abundance))) > 3) & (length(which(lls.data$Abundance[!is.na(lls.data$Abundance)] > 0)) > 0) ){
###
## This is measuring the non-zero minimum
cmin = min(lls.data$Abundance[which(lls.data$Abundance>0)],na.rm=T)
## I don't understand how cmin would be NA after their preprocessing steps? Should be at least 1 non-zero values?
## I suppose this is their way of transforming so that they can take the log of the data
if (is.na(cmin)){
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5
} else {
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5 * cmin #replace zeroes with 0.5*minimum abundance value in this time series
}
lls.data$Abundance = log(lls.data$Abundance) #log-transform abundances
if (length(which(lls.data$Abundance==0))==length(lls.data$Abundance)){ #abundances are all = 1
add.coef = add.coef.Z = c(NA,NA,0,0,NA,NA,NA)
} else {
lls.years = sort(lls.data$Year)
lls.years.stretch = seq(lls.years[1],lls.years[length(lls.years)],1) #expand time series to include missing years
llsy.match = match(lls.years.stretch,lls.years)
X = llsy.match
## Here they are replacing the years with abundances and then having NAs for the years missing data in the expanded set
X[which(!is.na(X))] = lls.data$Abundance[X[which(!is.na(X))]] #fill-in abundance values in the expanded time series
## For some reason R is not allowing me to keep the format the same as they had it, so instead of X - mean() I am
## trying -mean() + X
Z = (-mean(X, na.rm=T) + X)/sd(X, na.rm=T) # z-transform
t.scale = 1:length(lls.years.stretch)
## Here they are standardizing timescales, I assume to make so that it is the same across different lengths of monitoring.
## It says it is transforming it between 0 and 1 but I think that it would need to be divided by max(t.scale) - min(t.scale)
# for that to be the case. Instead its transforming to ~almost~ 1.
t.scale = (t.scale-min(t.scale))/max(t.scale) #original transform: scale between 0 and 1
arr.Z = AR_reml(Z ~ t.scale) #Z-transformed time trends
}
time.trends.Z[tx,1] = strsplit(u.LLS2[l],'_')[[1]][1] #LTER
time.trends.Z[tx,2] = strsplit(u.LLS2[l],'_')[[1]][2] #Site
time.trends.Z[tx,3] = paste0(strsplit(u.LLS2[l],'_')[[1]][3:length(strsplit(u.LLS2[l],'_')[[1]])],collapse='_') #Species
time.trends.Z[tx,4] = add.coef.Z[1] #MSE
time.trends.Z[tx,5] = add.coef.Z[2] #b
time.trends.Z[tx,6] = add.coef.Z[3] #coef_int
time.trends.Z[tx,7] = add.coef.Z[4] #coef_slope
time.trends.Z[tx,8] = add.coef.Z[5] #Pr_int
time.trends.Z[tx,9] = add.coef.Z[6] #Pr_slope
time.trends.Z[tx,10] = add.coef.Z[7] #logLik
time.trends.Z[tx,11] = length(which(!is.na(Z))) #length of time series
time.trends.Z[tx,12] = lls.years.stretch[1] #first year in time series
time.trends.Z[tx,13] = lls.years.stretch[length(lls.years.stretch)] #last year in time series
time.trends.Z[tx,14] = lls.data$Feeding[1]
time.trends.Z[tx,15] = lls.data$Habitat[1]
time.trends.Z[tx,16] = lls.data$Pollinator[1]
time.trends.Z[tx,17] = lls.data$Order[1]
tx = tx + 1
# Plot high quality time series
png(paste0(gsub('\\?','',u.LLS[l]),'.png'))
plot(t.scale,Z,type='l',main=u.LLS2[l],xlab='Scaled time',ylab='Z-transformed abundance',lwd=2)
abline(a=add.coef.Z[3],b=add.coef.Z[4],lty=2,col='red',lwd=1.5)
legend('topright',legend=c(paste0('slope = ',round(add.coef.Z[4],2)),paste0('length = ',length(which(!is.na(Z)))),paste0('autocor = ',round(add.coef.Z[2],2)),paste0('logLik = ',round(add.coef.Z[7],2))),ncol=1)
dev.off()
} else {
#ignore low quality time series
}
}
time.trends.Z = data.frame('LTER'=NA,'Site'=NA,'Species'=NA,'MSE'=-999,'b'=-999,'coef_int'=-999,'coef_slope'=-999,'Pr_int'=-999,'Pr_slope'=-999,'logLik'=-999,'length'=-999,'Y1'=NA,'Y2'=NA,'Feeding'=NA,'Habitat'=NA,'Pollinator'=NA,'Order'=NA)
tx = 1
# this goes through each species in each site
for (l in 1:length(u.LLS2)){
print(l)
lls.data = data1[which(LLS==u.LLS2[l]),]
## This part I added in to try and keep the data to just the 2015 data used in Crossley.
# lls.data = lls.data[which(lls.data$Year <= 2015), ]
## It seems like this is just recreating the site and study name? Not sure why
LL = paste0(strsplit(u.LLS2[l],'_')[[1]][1:2],collapse='_') #trim "no sample" years
## Removed a harvard forest ants specific processing step
### Quality threshold
## Checking if there are more than 3 years where there are actual measurements for abundance and that of those non-NA
## years there are at least a few with non-zero values for abundance measured.
if ( (length(which(!is.na(lls.data$Abundance))) > 3) & (length(which(lls.data$Abundance[!is.na(lls.data$Abundance)] > 0)) > 0) ){
###
## This is measuring the non-zero minimum
cmin = min(lls.data$Abundance[which(lls.data$Abundance>0)],na.rm=T)
## I don't understand how cmin would be NA after their preprocessing steps? Should be at least 1 non-zero values?
## I suppose this is their way of transforming so that they can take the log of the data
if (is.na(cmin)){
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5
} else {
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5 * cmin #replace zeroes with 0.5*minimum abundance value in this time series
}
lls.data$Abundance = log(lls.data$Abundance) #log-transform abundances
if (length(which(lls.data$Abundance==0))==length(lls.data$Abundance)){ #abundances are all = 1
add.coef = add.coef.Z = c(NA,NA,0,0,NA,NA,NA)
} else {
lls.years = sort(lls.data$Year)
lls.years.stretch = seq(lls.years[1],lls.years[length(lls.years)],1) #expand time series to include missing years
llsy.match = match(lls.years.stretch,lls.years)
X = llsy.match
## Here they are replacing the years with abundances and then having NAs for the years missing data in the expanded set
X[which(!is.na(X))] = lls.data$Abundance[X[which(!is.na(X))]] #fill-in abundance values in the expanded time series
## For some reason R is not allowing me to keep the format the same as they had it, so instead of X - mean() I am
## trying -mean() + X
Z = (-mean(X, na.rm=T) + X)/sd(X, na.rm=T) # z-transform
t.scale = 1:length(lls.years.stretch)
## Here they are standardizing timescales, I assume to make so that it is the same across different lengths of monitoring.
## It says it is transforming it between 0 and 1 but I think that it would need to be divided by max(t.scale) - min(t.scale)
# for that to be the case. Instead its transforming to ~almost~ 1.
t.scale = (t.scale-min(t.scale))/max(t.scale) #original transform: scale between 0 and 1
arr.Z = AR_reml(Z ~ t.scale) #Z-transformed time trends
add.coef.Z = c(arr.Z[[1]],arr.Z[[2]],arr.Z[[3]][1,1],arr.Z[[3]][2,1],arr.Z[[5]][1],arr.Z[[5]][2],arr.Z[[6]])
}
time.trends.Z[tx,1] = strsplit(u.LLS2[l],'_')[[1]][1] #LTER
time.trends.Z[tx,2] = strsplit(u.LLS2[l],'_')[[1]][2] #Site
time.trends.Z[tx,3] = paste0(strsplit(u.LLS2[l],'_')[[1]][3:length(strsplit(u.LLS2[l],'_')[[1]])],collapse='_') #Species
time.trends.Z[tx,4] = add.coef.Z[1] #MSE
time.trends.Z[tx,5] = add.coef.Z[2] #b
time.trends.Z[tx,6] = add.coef.Z[3] #coef_int
time.trends.Z[tx,7] = add.coef.Z[4] #coef_slope
time.trends.Z[tx,8] = add.coef.Z[5] #Pr_int
time.trends.Z[tx,9] = add.coef.Z[6] #Pr_slope
time.trends.Z[tx,10] = add.coef.Z[7] #logLik
time.trends.Z[tx,11] = length(which(!is.na(Z))) #length of time series
time.trends.Z[tx,12] = lls.years.stretch[1] #first year in time series
time.trends.Z[tx,13] = lls.years.stretch[length(lls.years.stretch)] #last year in time series
time.trends.Z[tx,14] = lls.data$Feeding[1]
time.trends.Z[tx,15] = lls.data$Habitat[1]
time.trends.Z[tx,16] = lls.data$Pollinator[1]
time.trends.Z[tx,17] = lls.data$Order[1]
tx = tx + 1
# Plot high quality time series
png(paste0(gsub('\\?','',u.LLS[l]),'.png'))
plot(t.scale,Z,type='l',main=u.LLS2[l],xlab='Scaled time',ylab='Z-transformed abundance',lwd=2)
abline(a=add.coef.Z[3],b=add.coef.Z[4],lty=2,col='red',lwd=1.5)
legend('topright',legend=c(paste0('slope = ',round(add.coef.Z[4],2)),paste0('length = ',length(which(!is.na(Z)))),paste0('autocor = ',round(add.coef.Z[2],2)),paste0('logLik = ',round(add.coef.Z[7],2))),ncol=1)
dev.off()
} else {
#ignore low quality time series
}
}
head(time.trends.Z)
ggplot(time.trends.Z, aes(LTER, coef_slope)) + geom_violin()
u.LLS2
u.LLS2[1]
l = 1
lls.data = data1[which(LLS==u.LLS2[l]),]
## This part I added in to try and keep the data to just the 2015 data used in Crossley.
# lls.data = lls.data[which(lls.data$Year <= 2015), ]
## It seems like this is just recreating the site and study name? Not sure why
LL = paste0(strsplit(u.LLS2[l],'_')[[1]][1:2],collapse='_') #trim "no sample" years
lls.data$Abundance = log(lls.data$Abundance) #log-transform abundances
lls.years = sort(lls.data$Year)
lls.years.stretch = seq(lls.years[1],lls.years[length(lls.years)],1) #expand time series to include missing years
llsy.match = match(lls.years.stretch,lls.years)
X = llsy.match
## Here they are replacing the years with abundances and then having NAs for the years missing data in the expanded set
X[which(!is.na(X))] = lls.data$Abundance[X[which(!is.na(X))]] #fill-in abundance values in the expanded time series
## For some reason R is not allowing me to keep the format the same as they had it, so instead of X - mean() I am
## trying -mean() + X
Z = (-mean(X, na.rm=T) + X)/sd(X, na.rm=T) # z-transform
t.scale = 1:length(lls.years.stretch)
X
Z
mean(X)
mean(X, na.rm = T)
t.scale
t.scale = (t.scale-min(t.scale))/max(t.scale) #original transform: scale between 0 and 1
t.scale
slope_plc = as.numeric(lm(Z ~ t.scale)[[1]][2])
slope_plc
plot(Z ~ t.scale)
abline(lm(Z ~ t.scale))
time.trends = data.frame('LTER'=NA,'Site'=NA,'Species'=NA, 'coef_slope'=-999)
time.trends
LLS = paste(data1$LTER.site,data1$Locale,data1$Species.code,sep='_') #unique LTER by sub-site by species combinations
u.LLS = unique(LLS)
LLS.count = apply(array(u.LLS),1,function(x){d2=data1[which(LLS==x),];length(which(d2$Abundance!=0))})
u.LLS2 = u.LLS[which(LLS.count>0)]
time.trends = data.frame('LTER'=NA,'Site'=NA,'Species'=NA, 'coef_slope'=-999)
tx = 1
# this goes through each species in each site
for (l in 1:length(u.LLS2)){
print(l)
lls.data = data1[which(LLS==u.LLS2[l]),]
## This part I added in to try and keep the data to just the 2015 data used in Crossley.
# lls.data = lls.data[which(lls.data$Year <= 2015), ]
## It seems like this is just recreating the site and study name? Not sure why
LL = paste0(strsplit(u.LLS2[l],'_')[[1]][1:2],collapse='_') #trim "no sample" years
## Removed a harvard forest ants specific processing step
### Quality threshold
## Checking if there are more than 3 years where there are actual measurements for abundance and that of those non-NA
## years there are at least a few with non-zero values for abundance measured.
if ( (length(which(!is.na(lls.data$Abundance))) > 3) & (length(which(lls.data$Abundance[!is.na(lls.data$Abundance)] > 0)) > 0) ){
###
## This is measuring the non-zero minimum
cmin = min(lls.data$Abundance[which(lls.data$Abundance>0)],na.rm=T)
## I don't understand how cmin would be NA after their preprocessing steps? Should be at least 1 non-zero values?
## I suppose this is their way of transforming so that they can take the log of the data
if (is.na(cmin)){
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5
} else {
lls.data$Abundance[which(lls.data$Abundance==0)] = 0.5 * cmin #replace zeroes with 0.5*minimum abundance value in this time series
}
lls.data$Abundance = log(lls.data$Abundance) #log-transform abundances
if (length(which(lls.data$Abundance==0))==length(lls.data$Abundance)){ #abundances are all = 1
slope_plc = 0
} else {
lls.years = sort(lls.data$Year)
lls.years.stretch = seq(lls.years[1],lls.years[length(lls.years)],1) #expand time series to include missing years
llsy.match = match(lls.years.stretch,lls.years)
X = llsy.match
## Here they are replacing the years with abundances and then having NAs for the years missing data in the expanded set
X[which(!is.na(X))] = lls.data$Abundance[X[which(!is.na(X))]] #fill-in abundance values in the expanded time series
## For some reason R is not allowing me to keep the format the same as they had it, so instead of X - mean() I am
## trying -mean() + X
Z = (-mean(X, na.rm=T) + X)/sd(X, na.rm=T) # z-transform
t.scale = 1:length(lls.years.stretch)
## Here they are standardizing timescales, I assume to make so that it is the same across different lengths of monitoring.
## It says it is transforming it between 0 and 1 but I think that it would need to be divided by max(t.scale) - min(t.scale)
# for that to be the case. Instead its transforming to ~almost~ 1.
t.scale = (t.scale-min(t.scale))/max(t.scale) #original transform: scale between 0 and 1
slope_plc = as.numeric(lm(Z ~ t.scale)[[1]][2])
}
time.trends[tx,1] = strsplit(u.LLS2[l],'_')[[1]][1] #LTER
time.trends[tx,2] = strsplit(u.LLS2[l],'_')[[1]][2] #Site
time.trends[tx,3] = paste0(strsplit(u.LLS2[l],'_')[[1]][3:length(strsplit(u.LLS2[l],'_')[[1]])],collapse='_') #Species
time.trends[tx,4] =slope_plc
tx = tx + 1
# Plot high quality time series
png(paste0(gsub('\\?','',u.LLS[l]),'.png'))
plot(t.scale,Z,type='l',main=u.LLS2[l],xlab='Scaled time',ylab='Z-transformed abundance',lwd=2)
abline(a=add.coef.Z[3],b=add.coef.Z[4],lty=2,col='red',lwd=1.5)
legend('topright',legend=c(paste0('slope = ',round(add.coef.Z[4],2)),paste0('length = ',length(which(!is.na(Z)))),paste0('autocor = ',round(add.coef.Z[2],2)),paste0('logLik = ',round(add.coef.Z[7],2))),ncol=1)
dev.off()
} else {
#ignore low quality time series
}
}
head(time.trends)
ggplot(data = time.trends, aes(LTER, coef_slope)) + geom_violin()
time.trends$LTER = 'no ar_reml'
time.trends
all_trends = rbind(time.trends, cbind(time.trends.Z[1], time.trends.Z[2], time.trends.Z[3], time.trends.Z[7]))
all_trends
ggplot(data = all_trends, aes(LTER, coef_slope)) + geom_violin()
data1 = read.csv('PerSpecies_Abundance_LTER_annotated.csv',as.is=T,check.names=F,header=T)
data1$LL = paste(data1$LTER.site,data1$Locale,sep='_')
head(data1)
m_dat = read.csv(file = "/Users/caitlinmiller/Desktop/Github/InsectTrends/data/Model1_sampledAbundance_RAW.csv", header = FALSE)
# They log transform all files so first we will do the same.
m_dat_l = log(m_dat)
slopes = rep(NA, ncol(m_dat_l))
# Next we will go through each column and transform values based on the mean and standard deviation of the species, and then use AR_reml()
# to derive a value for slope.
for(sp in 1:ncol(m_dat_l))
{
X = m_dat_l[,sp]
Z = (X - mean(X))/sd(X)
t_scale = seq(from = 0, to = 1, length.out = nrow(m_dat_l))
slopes[sp] = AR_reml(Z ~ t_scale)[[3]][2,1]
}
all_dat = data.frame(LTER = rep('mod', length(slopes)), slopes)
# Now trying to do an analysis where we just take a linear model of each column and then take the slope of the regression as the slope of change
slopes = rep(NA, ncol(m_dat))
times = 1:nrow(m_dat)
for(sp in 1:ncol(m_dat))
{
abuns = m_dat[,sp]
slopes[sp] = as.numeric(lm(abuns ~ times)[[1]][2])
}
all_dat = rbind(all_dat, cbind(LTER = rep('norm', length(slopes)), slopes))
# Now something that only log transforms data because our data does drop so suddenly it is exponential.
slopes = rep(NA, ncol(m_dat_l))
times = 1:nrow(m_dat_l)
for(sp in 1:ncol(m_dat_l))
{
abuns = m_dat_l[,sp]
slopes[sp] = as.numeric(lm(abuns ~ times)[[1]][2])
}
all_dat = rbind(all_dat, cbind(LTER = rep("log", length(slopes)), slopes))
all_dat$slopes = as.numeric(all_dat$slopes)
ggplot(data = all.dat, aes(LTER, mod_slopes)) + geom_violin()
ggplot(data = all_dat, aes(LTER, mod_slopes)) + geom_violin()
ggplot(data = all_dat, aes(LTER, slopes)) + geom_violin()
m_dat = read.csv(file = "/Users/caitlinmiller/Desktop/Github/InsectTrends/data/Model2_sampledAbundance_RAW.csv", header = FALSE)
# They log transform all files so first we will do the same.
m_dat_l = log(m_dat)
slopes = rep(NA, ncol(m_dat_l))
# Next we will go through each column and transform values based on the mean and standard deviation of the species, and then use AR_reml()
# to derive a value for slope.
for(sp in 1:ncol(m_dat_l))
{
X = m_dat_l[,sp]
Z = (X - mean(X))/sd(X)
t_scale = seq(from = 0, to = 1, length.out = nrow(m_dat_l))
slopes[sp] = AR_reml(Z ~ t_scale)[[3]][2,1]
}
all_dat = data.frame(LTER = rep('mod', length(slopes)), slopes)
# Now trying to do an analysis where we just take a linear model of each column and then take the slope of the regression as the slope of change
slopes = rep(NA, ncol(m_dat))
times = 1:nrow(m_dat)
for(sp in 1:ncol(m_dat))
{
abuns = m_dat[,sp]
slopes[sp] = as.numeric(lm(abuns ~ times)[[1]][2])
}
all_dat = rbind(all_dat, cbind(LTER = rep('norm', length(slopes)), slopes))
# Now something that only log transforms data because our data does drop so suddenly it is exponential.
slopes = rep(NA, ncol(m_dat_l))
times = 1:nrow(m_dat_l)
for(sp in 1:ncol(m_dat_l))
{
abuns = m_dat_l[,sp]
slopes[sp] = as.numeric(lm(abuns ~ times)[[1]][2])
}
all_dat = rbind(all_dat, cbind(LTER = rep("log", length(slopes)), slopes))
all_dat$slopes = as.numeric(all_dat$slopes)
ggplot(data = all_dat, aes(LTER, slopes)) + geom_violin()
which(all_dat$LTER == norm)
which(all_dat$LTER = norm)
which(all_dat$LTER == 'norm')
norm_i = which(all_dat$LTER == 'norm')
all_dat[norm_i, ]
mod_i = which(all_dat$LTER == 'mod')
all_dat[mod_i, ]
m_dat
m_dat[1,]
sum(m_dat[1,])
m_dat = read.csv(file = "/Users/caitlinmiller/Desktop/Github/InsectTrends/data/Model2_sampledAbundance_RAW.csv", header = FALSE)
abuns = rep(NA, nrow(m_dat))
for(r in 1:nrow(m_dat))
{
abuns[r] = sum(m_dat[r, ])
}
plot(1:nrow(m_dat), abuns)
plot(1:nrow(m_dat), abuns, type = 'l', xlab = 'Time steps', ylab = 'Total abundance')
plot(1:nrow(m_dat), abuns, type = 'l', xlab = 'Time steps', ylab = 'Total abundance', lwd = 2)
